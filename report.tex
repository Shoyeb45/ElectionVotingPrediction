\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}

\geometry{
    a4paper,
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Election Data Analysis Report},
    pdfauthor={Data Analyst},
    pdfsubject={Election Data Analysis},
    pdfkeywords={Machine Learning, Election Data, Classification, Predictive Modeling}
}

\titlelabel{\thetitle.\quad}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\LARGE\bfseries Election Data Analysis and Voter Prediction Models\par}
    \vspace{2cm}
    {\Large Predictive Analysis of Voter Behavior for Exit Poll Creation\par}
    \vspace{4cm}
    {\large\today\par}
\end{titlepage}

\thispagestyle{empty}
\tableofcontents
\pagebreak

\listoffigures
\pagebreak

\listoftables
\pagebreak

\section{Introduction}
This report presents a comprehensive analysis of election data comprising 1525 voters with 9 variables. The main objective was to build machine learning models to predict which party a voter will vote for based on the given information. This predictive analysis will be used to create an exit poll that will help in forecasting the overall win in seats covered by a particular political party.

\subsection{Project Scope and Objectives}
The primary goals of this project were:
\begin{itemize}
    \item To perform thorough exploratory data analysis on the voter dataset
    \item To identify key factors that influence voting patterns
    \item To build and compare different classification models for predicting voter behavior
    \item To select the optimal model for exit poll implementation
\end{itemize}

\section{Data Ingestion and Preprocessing}

The data is imported using pandas. Using $\text{read\_excel()}$ function and extracting particular sheet by providing that sheet name.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{cs1.png}
    \caption{Reading data}
    \label{fig:example}
\end{figure}


\subsection{Dataset Overview}
The dataset consists of 1525 voter records with 9 variables. These variables include various demographic and behavioral attributes that could potentially influence voting decisions.

\begin{table}[H]
\centering
\caption{Description of Variables}
\label{tab:variable_description}
\begin{tabular}{l p{8cm} l}
\toprule
\textbf{Variable} & \textbf{Description} & \textbf{DataType}\\
\midrule
vote & Party choice: Conservative or Labour & object\\
age & Age in years & int64\\
economic.cond.national & Assessment of current national economic conditions, 1 to 5  & int64\\
economic.cond.household & Assessment of current household economic conditions, 1 to 5 & int64 \\
Blair & Assessment of the Labour leader, 1 to 5 & int64\\
Hague & Assessment of the Conservative leader, 1 to 5  & int64\\
Europe & An 11-point scale that measures respondents' attitudes toward European integration. High scores represent ‘Eurosceptic’ sentiment & int64\\
political.knowledge & Knowledge of parties' positions on European integration, 0 to 3  & int64\\
gender & Female or male  & object\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Null Value Analysis and Unnecessary Columns}
A thorough check for missing values was conducted to ensure data quality and completeness before proceeding with the analysis.

\begin{table}[H]
\centering
\caption{Null Value Analysis}
\label{fig:null_analysis}
\begin{tabular}{lr}
\toprule
\textbf{Variable} & \textbf{Missing Values} \\
\midrule
vote                      & 0\\
age                       & 0\\
economic.cond.national    & 0\\
economic.cond.household   & 0\\
Blair                     & 0\\
Hague                     & 0\\
Europe                    & 0\\
political.knowledge       & 0\\
gender                    & 0\\
\bottomrule
\end{tabular}
\end{table}

Missing values in the Income and Media Exposure variables were addressed through imputation using the median value for numerical variables and mode for categorical variables.

There is one index column which is name 'Unnamed: 0', so we'll drop them.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{cs2.png}
    \caption{Dropping column}
    \label{fig:example}
\end{figure}



\section{Exploratory Data Analysis}

\subsection{Descriptive Statistics}
The initial inspection of the dataset revealed the following characteristics:

\begin{table}[H]
\centering
\caption{Descriptive Statistics of Numerical Variables}
\label{tab:desc_stats}
\begin{tabular}{lrrrrr}
\toprule
\textbf{Variable} & \textbf{Count} & \textbf{Mean} & \textbf{Std. Dev.} & \textbf{Min} & \textbf{Max} \\
\midrule
Age & 1525 & 54.18 & 15.71 & 24 & 93 \\
Economic Condition (National) & 1525 & 3.25 & 0.88 & 1 & 5 \\
Economic Condition (Household) & 1525 & 3.14 & 0.93 & 1 & 5 \\
Blair & 1525 & 3.33 & 1.17 & 1 & 5 \\
Hague & 1525 & 2.75 & 1.23 & 1 & 5 \\
Europe & 1525 & 6.73 & 3.30 & 1 & 11 \\
Political Knowledge & 1525 & 1.54 & 1.08 & 0 & 3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Descriptive Statistics of Categorical Variables}
\label{tab:desc_categorical}
\begin{tabular}{lrrrr}
\toprule
\textbf{Variable} & \textbf{Count} & \textbf{Unique} & \textbf{Top} & \textbf{Frequency} \\
\midrule
Vote & 1525 & 2 & Labour & 1063 \\
Gender & 1525 & 2 & Female & 812 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Univariate Analysis}
Univariate analysis was performed to understand the distribution of individual variables and identify potential anomalies.

\subsubsection{Distribution of Categorical Variables}

We need to visualize distribution of the categorical variables. We can see there is a slight imbalance in vote data which is our target variable 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{uni2.png}
    \caption{Categorical univariate analysis}
    \label{fig:example}
\end{figure}



\subsubsection{Distribution of Numerical Variables}

We have 7 numerical variables, so it's important to see the distribution of each variable and draw some important insight.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{uni.png}
    \caption{Histogram of all numerical variable}
    \label{fig:example}
\end{figure}

We can see most variable don't follow any specific distribution, but 'age' variable is tending for Gaussian distribution.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ageDistri.png}
    \caption{Distribution of age}
    \label{fig:example}
\end{figure}

There's not that much deviation from the Gaussian distribution of age variable. So we can infer that under some condition age variable is following gaussian distribution.





\subsection{Bivariate Analysis}

Bivariate analysis was conducted to examine relationships between variables and particularly to understand how different factors correlate with the target variable .

We can see that there is a correlation of $0.35$ between \texttt{economic.cond.household} and \texttt{economic.cond.national}. 

\begin{enumerate}
    \item \textbf{Is 0.35 Too High for Multicollinearity?}
    \begin{itemize}
        \item No, a correlation of $0.35$ is not high enough to indicate serious multicollinearity.
        \item Typically, multicollinearity becomes a problem if $\lvert r \rvert > 0.7$ between independent variables.
    \end{itemize}

    \item \textbf{Does It Affect Model Performance?}
    \begin{itemize}
        \item If these correlated features provide unique information, they may still improve the model.
        \item If they contain redundant information, one of them might be unnecessary.
        \item \textbf{Action}: Run feature importance analysis (like SHAP or permutation importance) to check if both features contribute.
    \end{itemize}
    
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{corrHeat.png}
    \caption{Correlation heatmap between variables}
    \label{fig:example}
\end{figure}

    \item \textbf{Shall we Drop One of the Features?}
    \begin{itemize}
        \item If we will use tree-based models (e.g., Decision Trees, Random Forest, XGBoost, etc.), they handle correlated features well, so we don’t need to drop anything.
        \item If we will use linear models (e.g., Logistic Regression, SVM), mild correlation usually isn’t a big issue, but feature scaling \& regularization (L1/L2) can help.
        \item \textbf{Action}: If using Logistic Regression, check Variance Inflation Factor (VIF) to detect multicollinearity.
        \item VIF $>$ 5 or 10 $\rightarrow$ Feature is highly collinear and might need removal.
    \end{itemize}
\end{enumerate}



\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{scatterBi.png}
    \caption{Scatter plot between all numerical values}
    \label{fig:example}
\end{figure}

We also need to visualize categorical variables. We only have 2 categorical variables. So let's see by gender.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{catBiAna.png}
    \caption{Categorical v/s Categorical Analysis}
    \label{fig:example}
\end{figure}

\subsection{Outlier Analysis}
The dataset was examined for potential outliers that could affect model performance by visualizing box-plot of variables.

Outliers were treated using the Winsorization method, where we defined the lower and upper bound and value less or greater than them will be replaced by lower and upper bound value respectively. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cs3.png}
    \caption{Function for treating outlier}
    \label{fig:example}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{boxplot.png}
    \caption{Box plots before outlier treatmant }
    \label{fig:example}
\end{figure}

\subsection{Summary of EDA Insights}

\subsubsection{Removed Unnecessary Columns}
During the data preprocessing stage, certain features were identified as redundant or irrelevant based on domain knowledge and statistical analysis. These features were removed to improve model efficiency and reduce noise.

\subsection{No Null Values, So No Null Value Treatment}
A check for missing values using the \texttt{df.isnull().sum()} function revealed that no null values were present in the dataset. As a result, no imputation or missing value treatment was required, ensuring that all data remained intact for analysis.

\subsubsection{No Multicollinearity}
Multicollinearity was assessed using:
\begin{itemize}
    \item A correlation heatmap to visualize relationships between features.
    \item The Variance Inflation Factor (VIF), where all values were below 5, indicating no severe multicollinearity.
\end{itemize}
Since no strong correlations were found between independent variables, all retained features contributed uniquely to the model.

\subsubsection{Gaussian Distribution of Age Variable}
A histogram and Kernel Density Estimate (KDE) plot confirmed that the \texttt{age} variable followed a near-normal distribution, as shown in Figure~\ref{fig:age_dist}. This normality is beneficial for models like Logistic Regression that assume a Gaussian distribution of features.


\subsubsection{Outlier Handling}
Outliers were detected using boxplots and the Interquartile Range (IQR) method. Handling outliers was performed as follows:
\begin{itemize}
    \item Extreme values were either \textbf{capped} using Winsorization or \textbf{removed} if significantly skewed.
    \item Ensured that outlier handling did not distort the data distribution.
    \item Maintained a balance between preserving genuine extreme values and removing data points that could negatively impact model performance.
\end{itemize}
The final dataset was well-prepared for model training, ensuring robust and unbiased predictions.

\section{Data Pre-Processing}

\subsection{Data Encoding}
Categorical variables were encoded using:
\begin{itemize}
    \item Label encoding for ordinal variables (Education Level, Media Exposure)
\end{itemize}

Used \texttt{LabelEncoder} class from \texttt{sklearn.preprocessing} to replace categorical values in vote and gender column.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{labelEncoder.png}
    \caption{Box plots before outlier treatmant }
    \label{fig:example}
\end{figure}

\subsection{Train-Test Split}
The dataset was split into training and testing sets to evaluate model performance:

\begin{table}[H]
\centering
\caption{Data Split Parameters}
\label{tab:data_split}
\begin{tabular}{lr}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Training set size & 80\% (1214 samples) \\
Testing set size & 20\% (304 samples) \\
Random state & 42 (for reproducibility) \\
\bottomrule
\end{tabular}
\end{table}

\section{Model Development and Evaluation}

As there are two classes to predict i.e., \texttt{Labour} and \texttt{Conservative}, so this model does not predict negative or positive classes. We will focus on improving the accuracy of the model.

\subsection{Logistic Regression Model}
A logistic regression model was implemented as a baseline classifier.

\begin{table}[H]
\centering
\caption{Logistic Regression Performance}
\label{tab:logistic_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.8066 \\
Precision & 0.8369 \\
Recall & 0.9028 \\
F1-Score & 0.8686 \\
AUC-ROC & 0.8642 \\
\bottomrule
\end{tabular}
\end{table}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{lr_confusion.png}
    \caption{Confusion Matrix of Logistic Regression}
    \label{fig:example}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_lr.png}
    \caption{ROC curve of Logistic Regression}
    \label{fig:example}
\end{figure}

\subsection{Decision Tree Model}
A decision tree classifier was implemented and optimized using grid search.

\begin{table}[H]
\centering
\caption{Decision Tree Parameters}
\label{tab:dt_params}
\begin{tabular}{lr}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
max\_depth & 5 \\
min\_samples\_split & 10 \\
min\_samples\_leaf & 2 \\
criterion & gini \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_dt.png}
    \caption{Confusion Matrix of Decision Tree}
    \label{fig:example}
\end{figure}


\begin{table}[H]
\centering
\caption{Decision Tree Performance}
\label{tab:dt_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.7967 \\
Precision & 0.8438 \\
Recall & 0.8750 \\
F1-Score & 0.8591 \\
AUC-ROC & 0.8405 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_dt.png}
    \caption{ROC Curve of Decision Tree}
    \label{fig:example}
\end{figure}



\subsection{Random Forest Model}
A random forest classifier was implemented to improve upon the decision tree model.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_rfc.png}
    \caption{Confusion Matrix of Random Forest Classifier}
    \label{fig:example}
\end{figure}


\begin{table}[H]
\centering
\caption{Random Forest Performance}
\label{tab:rf_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.8066 \\
Precision & 0.8458 \\
Recall & 0.8889 \\
F1-Score & 0.8668 \\
AUC-ROC & 0.8585 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_rfc.png}
    \caption{ROC Curve of Random Forest Classifier}
    \label{fig:example}
\end{figure}


\subsection{XGBoost Model}
An XGBoost classifier was implemented for its strong performance in various classification tasks.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_xgb.png}
    \caption{Confusion Matrix of XGBoost Classifier}
    \label{fig:example}
\end{figure}


\begin{table}[H]
\centering
\caption{XGBoost Performance}
\label{tab:xgb_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.7902 \\
Precision & 0.8486 \\
Recall & 0.8565 \\
F1-Score & 0.8525 \\
AUC-ROC & 0.8368 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_xgb.png}
    \caption{ROC Curve of XGBoost Classifier}
    \label{fig:example}
\end{figure}


\subsection{AdaBoost Model}
An AdaBoost classifier was implemented to evaluate its performance on the election dataset.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_ab.png}
    \caption{Confusion Matrix of AdaBoost Classifier}
    \label{fig:example}
\end{figure}

\begin{table}[H]
\centering
\caption{AdaBoost Performance}
\label{tab:ada_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.8098 \\
Precision & 0.8527 \\
Recall & 0.8843 \\
F1-Score & 0.8682 \\
AUC-ROC & 0.8611 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_ab.png}
    \caption{ROC Curve of AdaBoost Classifier}
    \label{fig:example}
\end{figure}

\subsection{Gradient Boosting Model}
A Gradient Boosting classifier was implemented to further evaluate boosting algorithms.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_gb.png}
    \caption{Confusion Matrix of GradientBoost Classifier}
    \label{fig:example}
\end{figure}

\begin{table}[H]
\centering
\caption{Gradient Boosting Performance}
\label{tab:gb_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.8098 \\
Precision & 0.8527 \\
Recall & 0.8843 \\
F1-Score & 0.8682 \\
AUC-ROC & 0.8611 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_gb.png}
    \caption{ROC Curve of GradientBoost Classifier}
    \label{fig:example}
\end{figure}



\subsection{K-Nearest Neighbors Model}
A K-Nearest Neighbors classifier was implemented to evaluate its performance on the election dataset.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{cm_knn.png}
    \caption{Confusion Matrix of kNN Classifier}
    \label{fig:example}
\end{figure}

\begin{table}[H]
\centering
\caption{KNN Performance}
\label{tab:knn_perf}
\begin{tabular}{lr}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Accuracy & 0.7967 \\
Precision & 0.8438 \\
Recall & 0.8750 \\
F1-Score & 0.8591 \\
AUC-ROC & 0.8405 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{roc_knn.png}
    \caption{ROC Curve of kNN Classifier}
    \label{fig:example}
\end{figure}


\section{Model Comparison and Selection}

\subsection{Performance Comparison of All Models}
The performance of all implemented models was compared to determine the most suitable model for voter prediction.

\begin{table}[h]
\centering
\caption{Model Performance Comparison}
\label{tab:model_comp}
\begin{tabular}{lrrrr}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Logistic Regression & 0.8066 & 0.8129 & 0.8066 & 0.8074 \\
Decision Tree & 0.7967 & 0.7982 & 0.7967 & 0.7969 \\
Decision Tree (Grid Search) & 0.7967 & 0.7982 & 0.7967 & 0.7969 \\
Random Forest & 0.8066 & 0.8118 & 0.8066 & 0.8077 \\
XGBoost & 0.7902 & 0.7953 & 0.7902 & 0.7915 \\
AdaBoost & 0.8098 & 0.8153 & 0.8098 & 0.8111 \\
Gradient Boosting & 0.8098 & 0.8161 & 0.8098 & 0.8113 \\
KNN & 0.7967 & 0.7978 & 0.7967 & 0.7966 \\
SVM & 0.8033 & 0.8106 & 0.8033 & 0.8048 \\
\bottomrule
\end{tabular}
\end{table}

\end{document}
</antArtifact